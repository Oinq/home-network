Initial system setup completed. Ubuntu Server 24.04.3 LTS installed and fully updated, using UEFI boot with separate `/boot` and `/boot/efi` partitions and an LVM layout (100 GB root, ~362 GB free for future volumes).

Hostname set to **erebor**.

Automatic security updates enabled via `unattended-upgrades`.

Base administration tools installed and verified: htop, lm-sensors, smartmontools, nvme-cli, iotop, iftop, ncdu, curl, wget, git, unzip.

Hardware validated (CPU, RAM, storage, sensors) with no kernel or hardware errors detected.

Basic local monitoring configured using Glances.

The system is now stable and ready for the next phase: installing the SAS HBA and adding the 12-disk storage subsystem.

Initial system setup completed. Ubuntu Server 24.04.3 LTS installed and fully updated, using UEFI boot with separate `/boot` and `/boot/efi` partitions and an LVM layout (100 GB root, ~362 GB free for future volumes).

Hostname set to **erebor**.

Automatic security updates enabled via `unattended-upgrades`.

Base administration tools installed and verified: htop, lm-sensors, smartmontools, nvme-cli, iotop, iftop, ncdu, curl, wget, git, unzip.

Hardware validated (CPU, RAM, storage, sensors) with no kernel or hardware errors detected.

Basic local monitoring configured using Glances.

The system is now stable and ready for the next phase: installing the SAS HBA and adding the 12-disk storage subsystem.

Glances monitoring was configured as a persistent systemd service,
bound to 0.0.0.0 and exposed to the local network.

The Home Assistant Glances integration was successfully connected
and is now ingesting live system metrics (CPU, memory, disks, temperatures, network, uptime, etc.).

The service is enabled at boot and verified operational after restart.

This establishes a functional monitoring baseline for the server,
ready for future expansion (alerts, dashboards, Prometheus/Grafana, filtering).

Minecraft server successfully migrated to the new host (erebor) using Docker.
Data was transferred via rsync into the new persistent directory under /srv/docker-data/minecraft.
Docker Engine and Docker Compose (plugin) were installed from the official Docker repository.
A fresh docker-compose.yml was created and the container (itzg/minecraft-server) started cleanly.
The container was validated through logs and health checks, and confirmed to restart correctly after reconnecting SSH.
This confirms that Docker, persistence, and service reliability are working correctly on the new system, independent of the RAID subsystem.

Minecraft server successfully migrated and validated after a full system reboot.
A real client successfully joined the world, confirming correct container startup, port exposure (25565), data persistence, and overall service stability.

Current setup is temporary:
The Minecraft data is currently stored at:

/srv/docker-data/minecraft/

This path is intentionally located on local system storage to allow early validation of the new server.
Once the RAID array is installed and mounted, this directory will be migrated to RAID-backed storage (e.g. under /mnt/raid/...) and the Docker bind mount paths will be updated accordingly.

This validates the migration method and confirms the new server is ready to progressively replace the old one.

Tailscale has been successfully configured on the Erebor server as a tailnet node with subnet routing enabled.
Erebor authenticates correctly, advertises the 192.168.1.0/24 route, and accepts routes from other peers.
Connectivity was validated from a remote node (Windows), confirming access both to Erebor’s Tailscale IP and to devices within the local LAN.
Remote access to the internal network is fully functional and stable.

Storage architecture was designed around real workload needs rather than theoretical edge cases.

The NVR uses JBOD (2TB + 2TB + 1TB), intentionally accepting data loss for camera recordings.

The NAS (Synology DS920+) uses 4 × 4TB in SHR (~12TB usable) and is positioned as secondary storage and backup, not as the primary source of truth.
The Erebor server concentrates the important data and performance-sensitive workloads.

On Erebor, the system and Docker engine run on a 500GB SSD, while Docker data is isolated on a dedicated 250GB SSD to keep write churn away from HDDs. 
A second 250GB SSD remains intentionally unassigned, reserved for future needs instead of being forced into an artificial role. 
A 320GB disk is used as scratch space for downloads, unpacking and temporary data.
Critical data is stored on a ZFS mirror (18TB + 18TB), ensuring redundancy where it actually matters.
Non-critical data on Erebor is stored on two independent 4TB disks (no pool, no JBOD, no RAID).
This is a deliberate choice: each disk stands alone, so the failure of one drive only affects the data stored on that specific disk, instead of causing total loss across a combined volume.
This approach favors fault isolation over capacity aggregation for data that is recoverable but still inconvenient to lose.

This layout intentionally leaves 5 of 12 bays free, preserving flexibility for future expansion instead of locking the system into premature pool designs.

